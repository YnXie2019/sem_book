---
title: "Latent Variable Modeling"
author: "Jon Lefcheck"
date: "November 13, 2018"
output: html_document
---

## 1.1 Introduction to Latent Variable Modeling

*Latent variables* are variables that are unobserved, but whose influence can be summarized through one or more *indicator variables*. They are useful for capturing complex or conceptual properties of a system that are difficult to quantify or measure directly. Early applications of latent variables, for example, focused on modeling the effects of 'general intelligence,' which is an abstract concept that is impossible to actually measure, but can be approximated using scores from different tests of cognitive performance (e.g., memory, verbal, spatial, etc.).

Consider the following simple example of a latent variable, in this case exogenous (informed only by predictors):

![latent variable](https://raw.githubusercontent.com/jslefche/sem_book/master/img/latent_variable_exo.png)

Here, the latent variable is indicated by the circle marked by $\xi$. The single indicator variable is indicated by the square box, as are all observed variables. You'll note a few curiosities compared to observed-variable models. 

First, the direction of causality is reversed from what you might expect: *from* the latent variables *to* the observed variable. This is because the indicator variable is simply an emergent manifestion of the underlying phenomenon represented by the latent variable.

Second, there is an error $\delta$ associated with the indicator. This implies that the indicator is an imperfect approximation of the latent construct. In other words, there are other emergent properties of the latent variable that affect the obsered indicator variables.

The latent variable can be related to the indicator variable using the following equation:

  $$x = \lambda \xi + \delta_{x}$$
  
In words, the values of $x$ are the result of the latent variable proportional to $\lambda$ (its effect on $x$) plus some error $\delta_{x}$.
  
A simple example of a latent-indicator relationship would be body size (latent) and body mass (indicator). There are obviously many aspects to body size that may be difficult to quanify, such as shape, volume, complexity, and so on. However, body mass is a simple, measurable consequence of these unmeasured characteristics, and thus can be thought to latently indicate body size. However, because we cannot perfectly measure body mass (there will always be some error associated with observed variables), we must incorporate that measurement error into our model of body size.

This example reinforces the point that latent variables often are used to represent concepts. Body size is often invoked in lots of ecological theory (e.g., metabolic theory, Bergmann's rule), but is almost always represented as some easily measurable quantity such as body mass rather than the complex, multidimensional construct that it is in reality. Latent variable modeling allows us to better approach that multidimensional construct by integrating a series of indicator variables that arise from the general concept of body size (e.g., mass, length, width, etc.). It therefore is a powerful tool that is better positioned to integrate theory and observation than past approaches.

However, some care should be taken when constructing latent variables. Just because we call a latent variable something does not always mean it *is* that thing For example, the latent variable body size as indicated by total abundance might appear legitimate--high abundances may constrain body sizes under limited resources--but is abundance *really* an indicator of this phenomenon? Can we go on to evaluate ecological theory about metabolic scaling on the basis of abundances? Probably not. So care should be taken when selecting/naming latent variables and identifying appropriate indicators. *Be sure the latent variable reflects the actual properties captured by the indicator variables!* The degree to which the indicators represent the phenomenon captured by the latent variable is termed *validity* and is often a qualitative approach to justifying the latent construct.

In contrast, *reliability* of the latent variable provides quantitative values with which to gauge how well an indicator reflects the latent variable. Reliability implies that the same values of the indicator would be obtained if they were continually resampled again and again. In other words, reliable indicators approach the true population mean that is the (theoretical) product of the latent variable: a perfect indicator would generate the same values every time so they would have a correlation $r = 1$. Of course, rarely do we sample the entire population or so well, and there will inevitably be some differences among our samples leading to deviations in this value away from 1. 

From this correlation, we can obtain a path coefficient from the latent to the indicator variable. Recall from the "Rules of Path Coefficients" (see Chapter: Global Estimation) the the coefficient on the path from the error variance $\zeta$ is the square-root of the unexplained variance (Rule 5). In this case, we want the opposite: we want the *shared* variance between the latent and indicator variable (a lot of shared variance is what makes a good indicator!). As in the case of the error path, the path coefficient from the latent variable to the indicator is often expressed in its standardized form: the square-root of the reliability. This value is also known as the *loading*.

From the reliability, we can also obtain the standardized error term $\delta_{x}$. This is the unshared variance, or 1 - the reliability. For the unstandardized form, one can apply the following equation:

  $$\delta_{x} = (1 - \lambda_{x}^2) \times VAR_{x}$$
As with other coefficients, standardization is applied simply because multiple indicators may be measured in vastly different units, and one may wish to fairly compare the loadings. 

Let's construct a simple example. Say we sample the variable $x$ repeatedly 5 times with $n = 10$. This could be 5 sampling dates or 5 separate trials.

```{r}
set.seed(11)

x <- rnorm(10)

x.list <- lapply(1:5, function(i) x + runif(10, 0, 2))
```

We can compute the average correlation among all trials. This is our measure of reliability:

```{r}
combos <- combn(1:5, 2)

cors <- c()

for(i in 1:ncol(combos)) cors <- c(cors, cor(x.list[[combos[1, i]]], x.list[[combos[2, i]]]))

(r <- mean(cors)) 
```

From this value, we can obtain the path coefficient and the (standardized error variance):

```{r}
sqrt(r) # path coefficient

1 - r # standardized error variance

(1 - r) * var(unlist(x.list)) # unstandardized error variance
```
  
In summary: the standardized coefficient (the loading) linking indicator to latent variables is the square-root of the relability. The standardized error variance is 1 - reliability.

So far, we have only dealt with latent variables as exogenous (predictor) variables, but they can also act as endogenous (response) variables. Here is an example endogenous latent variable:

![latent variable](https://raw.githubusercontent.com/jslefche/sem_book/master/img/latent_variable_endo.png)

The plot looks roughly similar, with some changes in the parameters: the error variance on $y$ is now $\epsilon_{y}$, while the latent variable itself is represented as $\eta$ and it has its own error $\zeta$. The presence of two errors presents a challenge: we simply don't have enough information to estimate all the unknowns here. 

In this case, we often assume no measurement error on $y$ such that $\epsilon_{y} = 0$. Consequently, $y$ becomes a prefect indicator of $\eta$ such that the reliability is total and $\lambda_{y} = 1$. We will get the calculation of $\zeta$ momentarily, which involves the value of the path(s) leading into $\eta$.

## 1.2 Application of Latent Variables to Path Models

Allowing both exogenous and endogenous latent variables now allows us to fit a *structural model*, or one with directed paths between latent variables. This is in contrast to a *measurement model*, which focuses solely on relating indicators to latent variables.

As an example of a structural model, let's combine the two latent variable models so that the exogenous latent variable is predicting the endogenous one: 

![latent structural model](https://raw.githubusercontent.com/jslefche/sem_book/master/img/latent_structural_model.png)

As before, let's fix the error of $y$ to be 0 so that the loading on $\eta = 1$. We can solve the exogenous paths as before, leaving us with two parameters left: the path coefficient $\gamma$ and $\zeta$.

We can solve the path coefficient $\gamma$ by knowing the estimated coefficient between $x$ and $y$ and adjusting by the loading of $x$ on $\epsilon$.

Let's return to our previous example and generate some data for $y$ then estimating the (standardized) coefficient:

```{r}
x <- unlist(x.list)

set.seed(11)

y <- rnorm(50)

xy_model <- lm(y ~ x)

beta <- summary(xy_model)$coefficients[2, 1]

beta_std <- beta * (sd(x) / sd(y)) # standardized
```

In this example, the estimated standardized path coefficient for $x$ on $y$ is $b = 0.31$.

We can obtain an estimate of gamma using the following equation:

  $$\gamma = \frac{b}{\lambda}$$
  
Which, for our example, is:

```{r}
(gamma <- beta_std / sqrt(r))
```

So the new estimate of the coefficient between the two latent variables is $\gamma = 0.34$. From this, we can obtain the unexplained variance, or $\epsilon$. Recall that the error $\delta_{x}$ is 1 - the explained variance, where the explained variance is the reliability. Here, we can transfer this knowledge such that: $\epsilon = 1 - \gamma^2$:

```{r}
1 - gamma^2

# compare to regression residual variance
1 - summary(xy_model)$r.squared
```

The error variance based on the model incorporated measurement error in $x$ has decreased. By removing the error in $x$ from our model, we have improved our estimate of the relationship between $x$ and $y$ *and* decreased the unexplained variance.

## 1.3 Latent Variables in *lavaan*

Let's reproduce this example using *lavaan*. The setup is almost identical except for a new operator `=~` which indicates a latent variable. Additionally, we will fix the variance in $x$ to the known variance from our repeated trials $r = 0.196$.

```{r}
library(lavaan)

latent_formula1 <- '
xi =~ x # exogenous latent
eta =~ y # endogenous latent

eta ~ xi # path model

x ~~ 0.196 * x # fix error variance
'

latent_model1 <- sem(latent_formula1, data.frame(x = x, y = y))

summary(latent_model1, standardize = T, rsq = T)
```

If we examine the output, we find a poor-fitting path model, but let's ignore that for now considering these were just random data. Instead, let's focus on the estimated parameters and compare them to our hand-calculated values. 

The standardized loading on $xi = 0.91$ which is very close to the value we calculated $r = 0.90$. The loading on $\eta = 1$. Notice how we didn't specify that: the default in *lavaan* is to set the first loading to 1. 

With respect to the regression coefficient, *lavaan* returned $\gamma = 0.341$ while we obtained $\gamma = 0.343$. Very close! Similarly the error variance on $\eta$ is 0.884, which is also nearly identical to $1 - \gamma^2 = 0.882$. Naturally, then, the explained variances are very close, being 1 - error variance.

So, all in all, for single indicator latent variables, we are able to almost exactly reproduce the output from *lavaan* (slight differences are likely due to the optimization algorithm). *lavaan*, however, provides an easier, more robust framework that easily extends to multi-indicator latent variables, and so we will use it from here on out.

## 1.4 Multi-indicator Latent Variables



confirmatory factor analysis

For the moment, latent variables are restricted to covariance-based SEM, although we are working to extend some concepts using the piecewise framework.
